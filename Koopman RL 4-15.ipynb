{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koopman RL Report: 04.15.2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import observables\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import numba as nb\n",
    "from scipy import integrate\n",
    "from estimate_L import *\n",
    "from cartpole_reward import cartpoleReward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brownian Updates: Condition number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Condition number of L:\", np.linalg.cond(L)) # inf\n",
    "print(\"Condition number of Psi_X:\", np.linalg.cond(Psi_X)) # 98 million+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "condition number of $\\hat{\\mathcal{L}}$ is $\\infty$\n",
    "\n",
    "condition number of $\\Psi_X$ is $98,000,000+$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced-Rank Regression (RRR)\n",
    "Using the fact that OLS is essentially orthogonal projectionon the column space of X, we can rewrite L as\n",
    "\\begin{align}\n",
    "    L = ||Y - X\\hat{B}_{\\text{OLS}}||^2 + ||X\\hat{B}_{\\text{OLS}} - XB||^2\n",
    "\\end{align}\n",
    "The first term does not depend on B and the second term can be minimized by the SVD/PCA of the fitted values $\\hat{Y} = X\\hat{B}_{\\text{OLS}}$\n",
    "Specifically, if $U_r$ are the first r prinicpal axes of $\\hat{Y}$, then\n",
    "\\begin{align}\n",
    "    \\hat{B}_{\\text{RRR}} = \\hat{B}_{\\text{OLS}} U_r U_r^\\top\n",
    "\\end{align}\n",
    "\n",
    "One can use it for regularization purposes. Similarly to ridge regression (RR), lasso, etc., RRR introduces some \"shrinkage\" penalty on B.\n",
    "The optimal rank r can be found via cross-validation. RRR easily outperforms OLS but tends to lose to RR.\n",
    "\n",
    "TODO: Understand connection to the reduced rank approach using SVD in databook for DMD (see 7.2) http://databookuw.com/databook.pdf. Both exploit Eckart-Young thm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(fastmath=True)\n",
    "def rrr(X, Y, rank=8):\n",
    "    B_ols = ols(X, Y)\n",
    "    U, S, V = np.linalg.svd(Y.T @ X @ B_ols)\n",
    "    W = V[0:rank].T\n",
    "\n",
    "    B_rr = B_ols @ W @ W.T\n",
    "    L = B_rr#.T\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Reward\n",
    "The default CartPole reward is always 1 until the episode terminates.\n",
    "We needed to modify this to make sense in the scope of our design so we found a variable reward formulation.\n",
    "We defined it below and put it into the CartPole environment for an agent to learn from to collect data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "x_threshold = 2.4\n",
    "gravity = 9.8\n",
    "masscart = 1.0\n",
    "masspole = 0.1\n",
    "total_mass = (masspole + masscart)\n",
    "length = 0.5  # actually half the pole's length\n",
    "polemass_length = (masspole * length)\n",
    "force_mag = 10.0\n",
    "tau = 0.02  # seconds between state updates\n",
    "kinematics_integrator = 'euler'\n",
    "\n",
    "# Angle at which to fail the episode\n",
    "theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "x_threshold = 2.4\n",
    "\n",
    "# Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "# is still within bounds.\n",
    "high = np.array([x_threshold * 2,\n",
    "                    np.finfo(np.float32).max,\n",
    "                    theta_threshold_radians * 2,\n",
    "                    np.finfo(np.float32).max],\n",
    "                dtype=np.float32)\n",
    "\n",
    "def cartpoleReward(state, action):\n",
    "    x, x_dot, theta, theta_dot = state\n",
    "\n",
    "    force = force_mag if action >= 0.5 else -force_mag\n",
    "    costheta = math.cos(theta)\n",
    "    sintheta = math.sin(theta)\n",
    "\n",
    "    temp = (force + polemass_length * theta_dot ** 2 * sintheta) / total_mass\n",
    "    thetaacc = (gravity * sintheta - costheta * temp) / (length * (4.0 / 3.0 - masspole * costheta ** 2 / total_mass))\n",
    "    xacc = temp - polemass_length * thetaacc * costheta / total_mass\n",
    "\n",
    "    if kinematics_integrator == 'euler':\n",
    "        x = x + tau * x_dot\n",
    "        x_dot = x_dot + tau * xacc\n",
    "        theta = theta + tau * theta_dot\n",
    "        theta_dot = theta_dot + tau * thetaacc\n",
    "    else:  # semi-implicit euler\n",
    "        x_dot = x_dot + tau * xacc\n",
    "        x = x + tau * x_dot\n",
    "        theta_dot = theta_dot + tau * thetaacc\n",
    "        theta = theta + tau * theta_dot\n",
    "\n",
    "    # done = bool(\n",
    "    #     x < -x_threshold\n",
    "    #     or x > x_threshold\n",
    "    #     or theta < -theta_threshold_radians\n",
    "    #     or theta > theta_threshold_radians\n",
    "    # )\n",
    "\n",
    "    reward = (1 - (x ** 2) / 11.52 - (theta ** 2) / 288)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important line from the above is the asignment\n",
    "$$reward = 1 - \\frac{x^2}{11.52} - \\frac{\\theta^2}{288} = 1 - \\frac{1}{2}\\left(\\frac{x}{2.4}\\right)^2 - \\frac{1}{2}\\left(\\frac{\\theta}{12}\\right)^2 $$\n",
    "\n",
    "The above takes 1 and subtracts the simple average of the normalized squared position and angle. We can see that with an increase in the absolute values of x and θ, the reward decreases and reaches 0 when |x| = 2.4 and |θ| = 12. Note that the angle and position in this reward function are functions themselves of the current action and previous state (angle, position, velocity, and angle velocity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Algorithms\n",
    "The following cell will run through the setup of the variables and functions necessary to run through the three algorithms.\n",
    "We used the Numba package in order to heavily reduce the compute time of various functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit(fastmath=True)\n",
    "def ln(x):\n",
    "    return np.log(x)\n",
    "@nb.njit(fastmath=True) #parallel=True,\n",
    "def nb_einsum(A, B):\n",
    "    assert A.shape == B.shape\n",
    "    res = 0\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            res += A[i,j]*B[i,j]\n",
    "    return res\n",
    "\n",
    "X = (np.load('random-cartpole-states.npy'))[:5000].T # states\n",
    "U = (np.load('random-cartpole-actions.npy'))[:5000].T # actions\n",
    "X_tilde = np.append(X, [U], axis=0) # extended states\n",
    "d = X_tilde.shape[0]\n",
    "m = X_tilde.shape[1]\n",
    "s = int(d*(d+1)/2) # number of second order poly terms\n",
    "\n",
    "psi = observables.monomials(2)\n",
    "Psi_X_tilde = psi(X_tilde)\n",
    "Psi_X_tilde_T = Psi_X_tilde.T\n",
    "k = Psi_X_tilde.shape[0]\n",
    "nablaPsi = psi.diff(X_tilde)\n",
    "nabla2Psi = psi.ddiff(X_tilde)\n",
    "\n",
    "@nb.njit(fastmath=True)\n",
    "def dpsi(X, k, l, t=1):\n",
    "    difference = X[:, l+1] - X[:, l]\n",
    "    term_1 = (1/t) * (difference)\n",
    "    term_2 = nablaPsi[k, :, l]\n",
    "    term_3 = (1/(2*t)) * np.outer(difference, difference)\n",
    "    term_4 = nabla2Psi[k, :, :, l]\n",
    "    return np.dot(term_1, term_2) + nb_einsum(term_3, term_4)\n",
    "\n",
    "# Construct \\text{d}\\Psi_X matrix\n",
    "dPsi_X_tilde = np.zeros((k, m))\n",
    "for row in range(k):\n",
    "    for column in range(m-1):\n",
    "        dPsi_X_tilde[row, column] = dpsi(X_tilde, row, column)\n",
    "dPsi_X_tilde_T = dPsi_X_tilde.T\n",
    "\n",
    "L = rrr(Psi_X_tilde_T, dPsi_X_tilde_T)\n",
    "\n",
    "@nb.njit\n",
    "def psi_x_tilde_with_diff_u(l, u):\n",
    "    result = Psi_X_tilde[:,l].copy()\n",
    "    result[-1] = u\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1\n",
    "We implemented the learning algorithm as outlined in our Koopman RL write-up\n",
    "\n",
    "The optimal value function $V$ satisfies the (regularized) Hamilton-Jacobi-Bellman (HJB) equation\n",
    "\\begin{align}\n",
    "    \\rho v(x) &= \\sup_{\\pi_t \\in \\mathcal{P}(U)} \\int_U \\left( r(x,u) - \\lambda \\ln{\\pi_t(u)} + (\\mathcal{L}v)(x,u)\\right) \\pi_t(u) \\text{d}u\n",
    "\\end{align}\n",
    "where $\\mathcal{P}(U) := \\{ \\pi_t:\\int_U \\pi_t(u) \\text{d}u = 1 \\text{ and } \\pi_t(u) \\geq 0 \\text{ a.e on } U \\}$. Projecting the value function onto the estimated eigensystem and reducing the dimension by choosing a cut off $c<n$ for the order of the eigenfunctions to include gives us an approximate characterization of optimality which we call **Koopman HJB**:\n",
    "\n",
    "\\begin{align}\n",
    "    \\rho v(x) &= \\sup_{\\pi_t \\in \\mathcal{P}(U)} \\int_U \\left (r(x,u) - \\lambda \\ln{\\pi_t(u)}\\right)\\pi_t(u) \\text{d}u + \\int_U \\sum_{\\ell = 1}^c \\langle \\widehat{\\varphi}_\\ell, v \\rangle \\widehat \\varphi_\\ell(x,u) \\pi_t(u) \\text{d}u \\label{inner_prod_approx}\n",
    "    \\\\\n",
    "    &= \\sup_{\\pi_t \\in \\mathcal{P}(U)} \\int_U \\left(r(x,u) - \\lambda \\ln{\\pi_t(u)} \\right)\\pi_t(u)\\text{d}u + \\sum_{\\ell = 1}^cm^v_\\ell\\lambda_\\ell\\int_U   \\widehat{\\varphi}_\\ell(x,u)  \\pi_t(u) \\text{d}u \\label{KoopmanHJB}\n",
    "\\end{align}\n",
    "\n",
    "Solving this maximization policy we get the feedback control:\n",
    "\\begin{align}\n",
    "    \\pi^*(u|x) &= \\frac{\\exp\\left(\\frac{1}{\\lambda}(r(x,u) + (\\mathcal{L}v)(x,u))\\right)}{\\int_U \\exp\\left(\\frac{1}{\\lambda}(r(x,u) + (\\mathcal{L}v)(x,u))\\right)du}\\notag\n",
    "    \\\\\n",
    "    &\\approx \\frac{\\exp\\left(\\frac{1}{\\lambda}(r(x,u) + \\sum_{\\ell = 1}^c m^v_\\ell\\lambda_\\ell \\widehat{\\varphi}_\\ell(x,u))  \\right)}{\\int_U \\exp\\left(\\frac{1}{\\lambda}(r(x,u) + \\sum_{\\ell = 1}^c m^v_\\ell\\lambda_\\ell \\widehat{\\varphi}_\\ell(x,u))  \\right)du} \\label{approxOptPolicy}\n",
    "    \\\\\n",
    "    &=: \\widehat{\\pi}^*(u | x, v)\n",
    "\\end{align}\n",
    "\n",
    "To run our algorithm, we initialize V in either some random way or we set it to 0. If we have some informed prior of what the form of the value function is, for example, using the assumption that the optimal value function is in the span of the dictionary space, we can probably speed up convergence significantly. \n",
    "\n",
    "Next, we want to find the OLS projection matrix $B^\\top_v$ of $V^{\\pi_0^*}$ onto the dictionary space by solving the following least squares problem \n",
    "\\begin{align}\n",
    "   \\min_{B_g}\\; \\lVert G_{\\tilde X} - B_g^\\top \\Psi_{\\tilde X}\\rVert_F \\label{gProjPsi}\n",
    "\\end{align}\n",
    "where $G_{\\tilde X} = (V(x_1), V(x_2),...,V(x_m))^\\top$ is the vector of the value function evaluated at the snapshots of the states not state action pairs $\\tilde x$. Note that we maintained this notation for generality since we may use $G$ to represent a matrix of observables at snapshots of the state action pairs. This notation will probably be revised because it may be confusing. \n",
    "\n",
    "Next, We approximate $\\mathcal{L}v$ in \n",
    "\\begin{align}\n",
    "    (\\mathcal{L}v(\\tilde{x})  \\approx \\sum_{\\ell = 1}^n \\lambda_\\ell \\widehat{\\varphi}_\\ell(\\tilde{x}) m^v\\ell \\label{est_eigen}\n",
    "\\end{align}\n",
    "which we then plug into \n",
    "\\begin{align}\n",
    "    \\frac{\\exp\\left(\\frac{1}{\\lambda}(r(x,u) + (\\mathcal{L}v)(x,u))\\right)}{\\int_U \\exp\\left(\\frac{1}{\\lambda}(r(x,u) + (\\mathcal{L}v)(x,u))\\right)du}\n",
    "\\end{align}\n",
    "to get our estimated optimal policy $\\hat{\\pi}^*(u | x, v)$\n",
    "\n",
    "Once we have our updated $\\hat{\\pi}^*(u | x, v)$, we can plug that into\n",
    "\\begin{align}\n",
    "    \\sup_{\\pi_t \\in \\mathcal{P}(U)} \\int_U \\left(r(x,u) - \\lambda \\ln{\\pi_t(u)} \\right)\\pi_t(u)\\text{d}u + \\sum_{\\ell = 1}^cm^v_\\ell\\lambda_\\ell\\int_U   \\widehat{\\varphi}_\\ell(x,u)  \\pi_t(u) \\text{d}u =: V^{\\pi_j^*}\n",
    "\\end{align}\n",
    "to get our updated $V^{\\pi_j^*}$\n",
    "\n",
    "We repeat this process for $t$ timesteps, specified by the caller of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningAlgorithm(L, X, Psi_X_tilde, U, reward, timesteps=100, cutoff=8, lamb=0.05):\n",
    "    # placeholder functions\n",
    "    V = lambda x: x\n",
    "    pi_hat_star = lambda x: x\n",
    "\n",
    "    low = np.min(U)\n",
    "    high = np.max(U)\n",
    "\n",
    "    constant = 1/lamb\n",
    "\n",
    "    eigenvalues, eigenvectors = sp.linalg.eig(L)\n",
    "    eigenvectors = eigenvectors\n",
    "    @nb.njit(fastmath=True)\n",
    "    def eigenfunctions(i, psi_x_tilde):\n",
    "        return np.dot(np.real(eigenvectors[i]), psi_x_tilde) #Psi_X_tilde[:, l]\n",
    "\n",
    "    eigenvectors_inverse_transpose = sp.linalg.inv(eigenvectors).T\n",
    "\n",
    "    currentV = np.zeros(X.shape[1]) # V^{\\pi*_0}\n",
    "    lastV = currentV.copy()\n",
    "    t = 0\n",
    "    while t < timesteps:\n",
    "        G_X_tilde = currentV.copy()\n",
    "        B_v = ols(Psi_X_tilde_T, G_X_tilde)\n",
    "\n",
    "        generatorModes = B_v.T @ eigenvectors_inverse_transpose\n",
    "\n",
    "        @nb.jit(forceobj=True, fastmath=True)\n",
    "        def Lv_hat(l, u):\n",
    "            psi_x_tilde = psi_x_tilde_with_diff_u(l, u)\n",
    "            summation = 0\n",
    "            for ell in range(cutoff):\n",
    "                summation += eigenvalues[ell] * eigenfunctions(ell, psi_x_tilde) * generatorModes[ell]\n",
    "            return summation\n",
    "\n",
    "        @nb.jit(forceobj=True, fastmath=True)\n",
    "        def compute(u, l):\n",
    "            inp = (constant * (reward(X[:,l], u) + Lv_hat(l, u))).astype('longdouble')\n",
    "            return np.exp(inp)\n",
    "\n",
    "        def pi_hat_star(u, l): # action given state\n",
    "            numerator = compute(u, l)\n",
    "            denominator = integrate.romberg(compute, low, high, args=(l,), divmax=30)\n",
    "            return numerator / denominator\n",
    "\n",
    "        def compute_2(u, l):\n",
    "            eval_pi_hat_star = pi_hat_star(u, l)\n",
    "            return (reward(X[:,l], u) - (lamb * ln(eval_pi_hat_star))) * eval_pi_hat_star\n",
    "\n",
    "        def integral_summation(l):\n",
    "            summation = 0\n",
    "            for ell in range(cutoff):\n",
    "                summation += generatorModes[ell] * eigenvalues[ell] * \\\n",
    "                    integrate.romberg(\n",
    "                        lambda u, l: eigenfunctions(ell, Psi_X_tilde[:, l]) * pi_hat_star(u, l),\n",
    "                        low, high, args=(l,), divmax=30\n",
    "                    )\n",
    "            return summation\n",
    "\n",
    "        def V(l):\n",
    "            return (integrate.romberg(compute_2, low, high, args=(l,), divmax=30) + \\\n",
    "                        integral_summation(l))\n",
    "\n",
    "        lastV = currentV\n",
    "        for i in range(currentV.shape[0]):\n",
    "            currentV[i] = V(i)\n",
    "\n",
    "        t+=1\n",
    "        print(\"Completed learning step\", t, \"\\n\")\n",
    "    \n",
    "    return currentV, pi_hat_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the results are a little strange. For one, the algorithm far too computationally complex, resulting in roughly a few minutes of compute per timestep.\n",
    "The other issue we were finding is that the optimal policy, regardless of the state, always has a slight preference to pick action 0 over action 1 (1.02 vs 0.98).\n",
    "This of course is incorrect as we would expect that the policy would prefer action 1 in the case where action 0 would cause you to terminate the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2\n",
    "Sinha et al. (https://arxiv.org/pdf/1909.12520.pdf) proposed an algorithm, termed Recursive EDMD, for learning the Koopman operator in an online learning setting.\n",
    "We propose an altered algorithm that allows us to retrieve the Koopman generator $\\mathcal{L}$ by making use of gEDMD in order to get its eigenfunctions and running the rest of the algorithm.\n",
    "\n",
    "Calculate components of $\\mathcal{L}_m$:\n",
    "\\begin{align}\n",
    "    z_m &\\gets z_{m-1} + \\text{d}\\Psi_{\\tilde{\\mathbf{X}}}\\Psi_{\\tilde{\\mathbf{X}}}^+\\\\\n",
    "    \\phi_m^{-1} &\\gets \\phi_{m-1}^{-1} - \\frac{\\phi_{m-1}^{-1} \\Psi_{\\tilde{\\mathbf{X}}} (\\Psi_{\\tilde{\\mathbf{X}}})^+ \\phi_{m-1}^{-1}}{1 + (\\Psi_{\\tilde{\\mathbf{X}}})^+ \\phi_{m-1}^{-1} \\Psi_{\\tilde{\\mathbf{X}}}}\n",
    "\\end{align}\n",
    "and then return $\\mathcal{L}_m \\gets z_m \\phi_m^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgEDMD(\n",
    "    x_tilde,\n",
    "    X_tilde,\n",
    "    Psi_X_tilde,\n",
    "    dPsi_X_tilde,\n",
    "    k,\n",
    "    z_m=np.zeros((k,k)),\n",
    "    phi_m_inverse=np.linalg.inv(np.identity(k))\n",
    "):\n",
    "    X_tilde = np.append(X_tilde, x_tilde.reshape(-1,1), axis=1)\n",
    "    Psi_X_tilde = psi(X_tilde)\n",
    "    for l in range(k):\n",
    "        dPsi_X_tilde[l, -1] = dpsi(X_tilde, l, -2)\n",
    "    dPsi_X_tilde = np.append(dPsi_X_tilde, np.zeros((k,1)), axis=1) #? should this really append 0s?\n",
    "\n",
    "    Psi_X_tilde_m = Psi_X_tilde[:,-1].reshape(-1,1)\n",
    "    Psi_X_tilde_m_T = Psi_X_tilde_m.T #? maybe pinv?\n",
    "\n",
    "    # update z_m\n",
    "    z_m = z_m + dPsi_X_tilde[:,-2].reshape(-1,1) @ Psi_X_tilde_m_T\n",
    "\n",
    "    # update \\phi_m^{-1}\n",
    "    phi_m_inverse = phi_m_inverse - \\\n",
    "                    ((phi_m_inverse @ Psi_X_tilde_m @ Psi_X_tilde_m_T @ phi_m_inverse) / \\\n",
    "                        (1 + Psi_X_tilde_m_T @ phi_m_inverse @ Psi_X_tilde_m))\n",
    "    \n",
    "    L_m = z_m @ phi_m_inverse\n",
    "\n",
    "    # updated dPsi_X_tilde, updated z_m, updated \\phi_m^{-1}, and approximate generator\n",
    "    return dPsi_X_tilde, z_m, phi_m_inverse, L_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 3\n",
    "Now that we can calculate the Koopman generator in an online learning setting by calling rgEDMD\n",
    "every time a new observation is made, we can, by extension, also run the learning algorithm in an online learning setting.\n",
    "The following is the combination of the two algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlineKoopmanLearning(X_tilde, Psi_X_tilde, dPsi_X_tilde):\n",
    "    X_tilde_builder = X_tilde[:,:2]\n",
    "    Psi_X_tilde_builder = Psi_X_tilde[:,:2]\n",
    "    dPsi_X_tilde_builder = dPsi_X_tilde[:,:2]\n",
    "    k = dPsi_X_tilde_builder.shape[0]\n",
    "\n",
    "    z_m = np.zeros((k,k))\n",
    "    phi_m_inverse = np.linalg.inv(np.identity(k))\n",
    "    for x_tilde in X_tilde.T: # for each data point\n",
    "        dPsi_X_tilde, z_m, phi_m_inverse, L_m = rgEDMD(\n",
    "            x_tilde, X_tilde_builder, Psi_X_tilde_builder, dPsi_X_tilde_builder, k, z_m, phi_m_inverse\n",
    "        ) # add new data point\n",
    "        _, pi = learningAlgorithm(L, X, Psi_X_tilde, np.array([0,1]), cartpoleReward, timesteps=2, lamb=1) # learn new optimal policy\n",
    "\n",
    "    # _, pi = learningAlgorithm(L, X, Psi_X_tilde, np.array([0,1]), cartpoleReward, timesteps=2, lamb=1)\n",
    "    return pi # esimated optimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current version of the algorithm, it might be infeasible to test since it has to run the computationally expensive learning algorithm every time a new data point is added.\n",
    "For testing purposes we can uncomment the line before \"return pi\" and comment out the looping learningAlgorithm call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Closure of Iteration Procedure in the Dictionary Space\n",
    "\n",
    "Here we would like to show that from one iteration to the next the updated value function still lies in the span of the dictionary functions. Let $H = span(\\psi)$ where we recall that $\\psi = (\\psi_1,...,\\psi_k)^\\top$. We first start with a value function $V^{\\pi^*_j}$ which we assume, along with the reward function $r(x,u)$ to be in $H$ as part of our induction assumption. This implies from the optimal policy expression above that we have\n",
    "\\begin{align}\n",
    "    \\widehat{\\pi}_{j+1}^*(u | x, v) = \\frac{\\exp\\left(\\sum_{\\ell = 1}^c \\alpha_{j,\\ell}\\widehat{\\varphi}_\\ell(x,u)\\right)  }{\\int_U \\exp\\left(\\sum_{\\ell = 1}^c \\alpha_{j,\\ell}\\widehat{\\varphi}_\\ell(x,u)\\right) du} \n",
    "\\end{align}\n",
    "\n",
    "Plugging this into the Koopman HJB above to get $V^{\\pi^*_{j+1}}$, it is unclear if $V^{\\pi^*_{j+1}}\\in H$, i.e. that the new value function remains in the span of the dictionary functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Operator Algebra Approach\n",
    "Let $\\mathcal{E}_x$ represent the conditional expectation over $\\pi^*(\\cdot|x)$, then we can represent the HJB expression as \n",
    "\\begin{align}\n",
    "    \\rho V  &= \\mathcal{E}_x r - \\mathcal{E}^*_x \\ln \\pi_t\n",
    "    \\\\\n",
    "    \\implies (\\rho I - \\mathcal{E}_x \\mathcal{L})V &= \\mathcal{E}_x(r - \\ln \\pi_t)\n",
    "    \\\\\n",
    "    \\implies \\ln \\pi^* &= \\left(\\rho\\mathcal{E}^{-1}_x -\\mathcal{L}\\right)V....\n",
    "\\end{align}\n",
    "\n",
    "Along the lines of operator analysis, as discussed with Wen, we would like to show that the overall procedure of projecting each iteration of the value function on the estimated eigensystem, finding the estimated optimal policy, and then finding the new value function results in a contraction map. If we assume that the optimal value function itself lies in the span of the dictionary space, it seems intuitive that this proceedure should converge to the optimal value function since each iteration some kind of composition between a projection operator and a Bellman operator, both of which are contractive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Mean Field Game Approaches Using Stochastic Maximum Principle\n",
    "There seem to be some promising works at the intersection of relaxed control theory and mean field games. Mixed strategies look very close to MDP problems and the way that MFGs are sometimes solved is with Pontryagin's maximum principle. See Daneil Lacker's thesis and IPAM summary papers ([Thesis Link](http://www.columbia.edu/~dl3133/dlacker-dissertation.pdf), [IPAM Lecture Link](http://www.columbia.edu/~dl3133/IPAM-MFGCompactnessMethods.pdf) [IPAM Lecture Video](http://www.ipam.ucla.edu/programs/summer-schools/graduate-summer-school-mean-field-games-and-applications/?tab=schedule))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
