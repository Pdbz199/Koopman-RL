{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('venv': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "3311e46391cba32dbf2fb02af71ea9c0ef298108ab432fcca8165679f38023c1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Koopman RL Report: 03.26.2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## OpenAI Gym Koopman Prediction Application: CartPole\n",
    "To see how well (and quickly) the Koopman operator could predict future states, I used a Q-learning agent that learned a near-optimal policy for the CartPole environment and fit an approximate Koopman operator to it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pykoopman as pk\n",
    "from pydmd import OptDMD\n",
    "import gym\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('state-action-inputs.npy') # 20,000 entries\n",
    "X = X[:int(X.shape[0]*0.0015)] # 30 points!\n",
    "\n",
    "# Fit Koopman operator using closed-form solution to DMD\n",
    "optdmd = OptDMD(svd_rank=15)\n",
    "model_optdmd = pk.Koopman(regressor=optdmd)\n",
    "model_optdmd.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "koopEnv = gym.make('CartPole-v0')\n",
    "\n",
    "Q_table = np.load('Q_table.npy')\n",
    "\n",
    "n_bins = ( 6, 12 )\n",
    "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
    "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
    "\n",
    "def discretizer( _, __, angle, pole_velocity ) -> Tuple[int,...]:\n",
    "    \"\"\"Convert continuous state into a discrete state\"\"\"\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([ lower_bounds, upper_bounds ])\n",
    "    return tuple( map( int, est.transform([[ angle, pole_velocity ]])[0] ) )\n",
    "\n",
    "def policy(state: tuple):\n",
    "    \"\"\" Choosing an action on epsilon-greedy policy \"\"\"\n",
    "    return np.argmax(Q_table[state])\n",
    "\n",
    "current_state = discretizer(*env.reset())\n",
    "current_stateK = discretizer(*koopEnv.reset())\n",
    "action = policy(current_state)\n",
    "actionK = policy(current_state)\n",
    "\n",
    "num_steps = 200\n",
    "q_learner_reward = 0\n",
    "koopman_reward = 0\n",
    "\n",
    "for i in range(num_steps):\n",
    "    # environment details\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    observationK, rewardK, doneK, _ = koopEnv.step(actionK)\n",
    "\n",
    "    # keep track of rewards\n",
    "    q_learner_reward += reward\n",
    "    koopman_reward += rewardK\n",
    "\n",
    "    # discretize state - hoping generator won't have to!\n",
    "    new_state = discretizer(*observation)\n",
    "    new_stateK = discretizer(*observationK)\n",
    "\n",
    "    # get actions\n",
    "    next_action = policy(new_state)\n",
    "    prediction = model_optdmd.predict(np.array([*list(current_stateK), actionK]))\n",
    "    prediction = np.round(np.real(prediction))\n",
    "    next_actionK = int(prediction[-1])\n",
    "\n",
    "    # update environments\n",
    "    action = next_action\n",
    "    actionK = next_actionK\n",
    "    current_state = new_state\n",
    "    current_stateK = new_stateK\n",
    "\n",
    "print(\"Q rewards:\", q_learner_reward)\n",
    "print(\"K rewards:\", koopman_reward)"
   ]
  },
  {
   "source": [
    "We can see that the rewards are both 200 which means that the Koopman predictor works very well given good data, though there are plenty of papers on the subject. One thing we may want to look into is how well we can learn a controller from the Koopman operator, but since we are focused on the Generator operator, what we really want to see now is the predictive power of the Koopman Generator and how it can be used for control!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stochastic Koopman Generator Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We simulated some paths from a standard Brownian Motion (drift coefficient 0 and diffusion coefficient 1) and then tried 3 different methods from two papers: \n",
    "+ Klus et al 2020 <https://arxiv.org/pdf/1909.10638.pdf>\n",
    "+ Li and Duan 2020 <https://arxiv.org/ftp/arxiv/papers/2005/2005.03769.pdf>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Simlulation of Brownian Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We simulated 20 paths of standard BM each with 5000 steps in a time interval of size 5000 so that the time step was 1. We took each of these 20 paths to be a state variable in our state vector. Our state vector is thus comprised of 20 iid BMs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Fitting the BM data using Generator EDMD (gEDMD)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brownian import brownian\n",
    "\n",
    "# The Diffusion process parameter.\n",
    "sigma = 1\n",
    "# Total time.\n",
    "T = 20000.0\n",
    "# Number of steps.\n",
    "N = 20000\n",
    "# Time step size\n",
    "dt = T/N\n",
    "# Number of realizations to generate.\n",
    "m = 20\n",
    "# Create an empty array to store the realizations.\n",
    "X = np.empty((m, N+1))\n",
    "# Initial values of x.\n",
    "X[:, 0] = 50\n",
    "brownian(X[:, 0], N, dt, sigma, out=X[:, 1:])\n",
    "Z = np.roll(X,-1)[:, :-1]\n",
    "X = X[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}