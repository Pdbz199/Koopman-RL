{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('Python36')",
   "metadata": {
    "interpreter": {
     "hash": "23ceee2e58b71c23a411bbe0797daaefa58b20287f71573072ea801f18122ee2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Koopman RL Report: 03.26.2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## OpenAI Gym Koopman Prediction Application: CartPole\n",
    "To see how well (and quickly) the Koopman operator could predict future states, I used a Q-learning agent that learned a near-optimal policy for the CartPole environment and fit an approximate Koopman operator to it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pykoopman as pk\n",
    "from pydmd import OptDMD\n",
    "import gym\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('state-action-inputs.npy') # 20,000 entries\n",
    "X = X[:int(X.shape[0]*0.0015)] # 30 points!\n",
    "\n",
    "# Fit Koopman operator using closed-form solution to DMD\n",
    "optdmd = OptDMD(svd_rank=15)\n",
    "model_optdmd = pk.Koopman(regressor=optdmd)\n",
    "model_optdmd.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "koopEnv = gym.make('CartPole-v0')\n",
    "\n",
    "Q_table = np.load('Q_table.npy')\n",
    "\n",
    "n_bins = ( 6, 12 )\n",
    "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
    "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
    "\n",
    "def discretizer( _, __, angle, pole_velocity ) -> Tuple[int,...]:\n",
    "    \"\"\"Convert continuous state into a discrete state\"\"\"\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([ lower_bounds, upper_bounds ])\n",
    "    return tuple( map( int, est.transform([[ angle, pole_velocity ]])[0] ) )\n",
    "\n",
    "def policy(state: tuple):\n",
    "    \"\"\" Choosing an action on epsilon-greedy policy \"\"\"\n",
    "    return np.argmax(Q_table[state])\n",
    "\n",
    "current_state = discretizer(*env.reset())\n",
    "current_stateK = discretizer(*koopEnv.reset())\n",
    "action = policy(current_state)\n",
    "actionK = policy(current_state)\n",
    "\n",
    "num_steps = 200\n",
    "q_learner_reward = 0\n",
    "koopman_reward = 0\n",
    "\n",
    "for i in range(num_steps):\n",
    "    # environment details\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    observationK, rewardK, doneK, _ = koopEnv.step(actionK)\n",
    "\n",
    "    # keep track of rewards\n",
    "    q_learner_reward += reward\n",
    "    koopman_reward += rewardK\n",
    "\n",
    "    # discretize state - hoping generator won't have to!\n",
    "    new_state = discretizer(*observation)\n",
    "    new_stateK = discretizer(*observationK)\n",
    "\n",
    "    # get actions\n",
    "    next_action = policy(new_state)\n",
    "    prediction = model_optdmd.predict(np.array([*list(current_stateK), actionK]))\n",
    "    prediction = np.round(np.real(prediction))\n",
    "    next_actionK = int(prediction[-1])\n",
    "\n",
    "    # update environments\n",
    "    action = next_action\n",
    "    actionK = next_actionK\n",
    "    current_state = new_state\n",
    "    current_stateK = new_stateK\n",
    "\n",
    "print(\"Q rewards:\", q_learner_reward)\n",
    "print(\"K rewards:\", koopman_reward)"
   ]
  },
  {
   "source": [
    "We can see that the rewards are both 200 which means that the Koopman predictor works very well given good data, though there are plenty of papers on the subject. One thing we may want to look into is how well we can learn a controller from the Koopman operator, but since we are focused on the Generator operator, what we really want to see now is the predictive power of the Koopman Generator and how it can be used for control!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stochastic Koopman Generator Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We simulated some paths from a standard Brownian Motion (drift coefficient 0 and diffusion coefficient 1) and then tried 3 different methods from two papers: \n",
    "+ Klus et al 2020 <https://arxiv.org/pdf/1909.10638.pdf>\n",
    "+ Li and Duan 2020 <https://arxiv.org/ftp/arxiv/papers/2005/2005.03769.pdf>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Simlulation of Brownian Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We simulated 20 paths of standard BM each with 5000 steps in a time interval of size 5000 so that the time step was 1. We took each of these 20 paths to be a state variable in our state vector. Our state vector is thus comprised of 20 iid BMs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Fitting the BM data using Generator EDMD (gEDMD)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brownian import brownian\n",
    "\n",
    "# The Diffusion process parameter.\n",
    "sigma = 1\n",
    "# Total time.\n",
    "T = 20000.0\n",
    "# Number of steps.\n",
    "N = 20000\n",
    "# Time step size\n",
    "dt = T/N\n",
    "# Number of realizations to generate.\n",
    "m = 20\n",
    "# Create an empty array to store the realizations.\n",
    "X = np.empty((m, N+1))\n",
    "# Initial values of x.\n",
    "X[:, 0] = 50\n",
    "brownian(X[:, 0], N, dt, sigma, out=X[:, 1:])\n",
    "Z = np.roll(X,-1)[:, :-1]\n",
    "X = X[:, :-1]"
   ]
  },
  {
   "source": [
    "### Fitting the BM data using Generator EDMD (gEDMD) from Klus et al. 2020"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From the learner's point of view, we assume that we are in the class of continuous Markov processes and thus that the generator is of the form\n",
    "$$\n",
    "    \\mathcal{L}f = b\\cdot\\nabla_{\\tilde x}f + \\frac{1}{2}a:\\nabla^2_{\\tilde x}f \n",
    "    %= \\sum_{i=1}^n b_i\\frac{\\partial f}{\\partial \\tilde{x}_i} + \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} \\frac{\\partial^2 f}{\\partial \\tilde{x}_i \\partial \\tilde{x}_j},\n",
    "$$\n",
    "where $a = \\sigma \\sigma^\\top$, $\\nabla^2_x$ denotes the Hessian, and $:$ denotes the double dot product. Applying the generator to each dictionary function \\psi_k and assuming that we have access to a single ergodic sample with time step $dt= 1$, we can use the following finite difference estimator of $d\\psi_k$:\n",
    "$$\n",
    "\\widehat{\\text{d}\\psi_k}(\\tilde{\\mathbf{x}}_l) = \\frac{1}{t}(\\tilde{\\mathbf{x}}_{l+1} - \\tilde{\\mathbf{x}}_l) \\cdot \\nabla\\psi_k(\\tilde{\\mathbf{x}}_l) + \\frac{1}{2t} \\Big[(\\tilde{\\mathbf{x}}_{l+1} - \\tilde{\\mathbf{x}}_l)(\\tilde{\\mathbf{x}}_{l+1} - \\tilde{\\mathbf{x}}_l)^\\top\\Big] : \\nabla^2 \\psi_k(\\tilde{\\mathbf{x}}_l)\n",
    "$$\n",
    "Note that we are adopting Klus's notation here only for reference. The stochastic total differential $d\\psi_k$ is a different object that the generator of the Koopman operator they are related in that the drift of the stochastic total differential is the same thing as the generator.\n",
    "\n",
    "The idea behind generator EDMD is that we assume that the genertor applied to the the dictionary functions can be (\"approximately\") expressed as a linear combination of the dictionary functions and find the coeficients of those linear combinations by minimizing $|| \\text{d}\\Psi_{\\tilde{\\mathbf{X}}} - M\\Psi_{\\tilde{\\mathbf{X}}} ||_F$ which leads to the least-squares approximation $$ M = \\text{d}\\Psi_{\\tilde{\\mathbf{X}}} \\Psi^{+}_{\\tilde{\\mathbf{X}}} = (\\text{d}\\Psi_{\\tilde{\\mathbf{X}}}\\Psi_{\\tilde{\\mathbf{X}}}^\\top)(\\Psi_{\\tilde{\\mathbf{X}}}\\Psi_{\\tilde{\\mathbf{X}}}^\\top)^+ $$\n",
    "\n",
    "Thus, we obtain the empirical estimate $L=M^T$ of the Koopman generator $\\mathcal{L}$.\n",
    "\n",
    "For the dictionary space, we chose monomials of up to order 2. We also tried monomials of order 1, which is not sufficient to pick up the diffusion term, but was successful at picking up the drift quicker. We hypothesize that this is because there are fewer terms in the regression over the dictionary functions.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import observables\n",
    "# from sympy import symbols\n",
    "# from sympy.polys.monomials import itermonomials, monomial_count\n",
    "# from sympy.polys.orderings import monomial_key\n",
    "\n",
    "# Construct B matrix as seen in 3.1.2 of the reference paper\n",
    "def constructB(d, k):\n",
    "    Bt = np.zeros((d, k))\n",
    "    if k == 1:\n",
    "        Bt[0,0] = 1\n",
    "    else:\n",
    "        num = np.arange(d)\n",
    "        Bt[num, num+1] = 1\n",
    "    B = Bt.T\n",
    "    return B\n",
    "\n",
    "# Construct similar B matrix as above, but for second order monomials\n",
    "def constructSecondOrderB(s, k):\n",
    "    Bt = np.zeros((s, k))\n",
    "    if k == 1:\n",
    "        Bt[0,0] = 1\n",
    "    else:\n",
    "        row = 0\n",
    "        for i in range(d+1, d+1+s):\n",
    "            Bt[row,i] = 1\n",
    "            row += 1\n",
    "    B = Bt.T\n",
    "    return B\n",
    "\n",
    "d = X.shape[0]\n",
    "m = X.shape[1]\n",
    "s = int(d*(d+1)/2) # number of second order poly terms\n",
    "rtoler=1e-02\n",
    "atoler=1e-02\n",
    "psi = observables.monomials(2)\n",
    "\n",
    "# x_str = \"\"\n",
    "# for i in range(d):\n",
    "#     x_str += 'x_' + str(i) + ', '\n",
    "# x_syms = symbols(x_str)\n",
    "# M = itermonomials(x_syms, 2)\n",
    "# sortedM = sorted(M, key=monomial_key('grlex', np.flip(x_syms)))\n",
    "# print(sortedM)\n",
    "\n",
    "Psi_X = psi(X)\n",
    "Psi_X_T = Psi_X.T\n",
    "nablaPsi = psi.diff(X)\n",
    "nabla2Psi = psi.ddiff(X)\n",
    "print(\"nablaPsi Shape\", nablaPsi.shape)\n",
    "k = Psi_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes dpsi_k(x) exactly as in the paper\n",
    "# t = 1 is a placeholder time step, not really sure what it should be\n",
    "def dpsi(k, l, t=1):\n",
    "    difference = (X[:, l+1] - X[:, l])\n",
    "    term_1 = (1/t) * (difference)\n",
    "    term_2 = nablaPsi[k, :, l]\n",
    "    term_3 = (1/(2*t)) * (difference.reshape(-1, 1) @ difference.reshape(1, -1))\n",
    "    term_4 = nabla2Psi[k, :, :, l]\n",
    "    return np.dot(term_1, term_2) + np.tensordot(term_3, term_4)\n",
    "vectorized_dpsi = np.vectorize(dpsi)\n",
    "\n",
    "# Construct \\text{d}\\Psi_X matrix\n",
    "dPsi_X = np.empty((k, m))\n",
    "for column in range(m-1):\n",
    "    dPsi_X[:, column] = vectorized_dpsi(range(k), column)\n",
    "\n",
    "# Calculate Koopman generator approximation\n",
    "train = int(m * 0.8)\n",
    "test = m - train\n",
    "M = dPsi_X[:, :train] @ np.linalg.pinv(Psi_X[:, :train]) # \\widehat{L}^\\top\n",
    "L = M.T # estimate of Koopman generator"
   ]
  }
 ]
}